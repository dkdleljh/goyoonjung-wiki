#!/usr/bin/env python3
"""Backfill alternative proofs for skipped-domain URLs that still lack alternatives.

Input
- data/alternative_proofs_missing.json (generated by scripts/audit_missing_alternative_proofs.py)

Approach (recommended/safe)
- No web fetch.
- No fact invention.
- Find alternative URLs that are ALREADY present elsewhere in the repo and are NOT skipped.

Heuristics
1) Same file: if the file already contains non-skipped URLs, use up to 3 as alternatives.
2) Works pages: if file is pages/works/<slug>.md, also look at related navigation pages:
   - pages/filmography.md
   - pages/schedule.md
   - pages/pictorials/by-year.md
   and include any non-skipped URLs that also mention the slug keyword in the same file.
3) Brand pages: pages/brands/<brand>.md may have other official links (instagram/youtube/newsroom) in the same file.

Insertion
- Same strategy as apply_alternative_proofs.py: insert a line after the skipped URL line
  when in a structured entry; else append an HTML comment.

Output
- Prints counts. Does not regenerate reports.
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path
from urllib.parse import urlsplit

BASE = Path(__file__).resolve().parent.parent
DATA = BASE / "data" / "alternative_proofs_missing.json"

URL_RE = re.compile(r"https?://[^\s)\]>\"']+")

SKIP_DOMAINS = {
    "namu.wiki",
    "nc.press",
    "kstarfashion.com",
    "program.tving.com",
    "tving.com",
    "disneyplus.com",
    "disneypluskr.com",
    "chanel.com",
    "boucheron.com",
    "about.netflix.com",
    "yna.co.kr",
    "goodal.com",
    "vodana.co.kr",
    "lensme.co.kr",
    "easytomorrow.co.kr",
    "youtube.com",
    "youtu.be",
    "instagram.com",
    "facebook.com",
    "twitter.com",
    "x.com",
}

ALT_MARKERS = ("대체 근거(열리는 링크):", "ALT-PROOF:")

ENTRY_START_RE = re.compile(r"^\s*-\s*(날짜|매체|구분|종류|제목|링크|링크\(원문\)|링크\(공식/원문\)|링크\(공식\)|프로그램/행사명|행사명)\s*:")
DATE_LINE_RE = re.compile(r"^\s*-\s*날짜\s*:\s*")
HEADING_RE = re.compile(r"^##\s+")


@dataclass(frozen=True)
class Missing:
    file: str
    line: int
    url: str


def host(url: str) -> str:
    return urlsplit(url).netloc.lower()


def is_skipped(url: str) -> bool:
    h = host(url)
    return any(h.endswith(d) for d in SKIP_DOMAINS)


def extract_urls(text: str) -> list[str]:
    return URL_RE.findall(text)


def has_alt_near(lines: list[str], idx0: int, window: int = 6) -> bool:
    a = max(0, idx0 - 1)
    b = min(len(lines), idx0 + window)
    for j in range(a, b):
        if any(m in lines[j] for m in ALT_MARKERS):
            return True
    return False


def in_structured_entry(lines: list[str], idx: int) -> bool:
    start = max(0, idx - 25)
    for j in range(idx, start - 1, -1):
        if HEADING_RE.match(lines[j].strip()):
            break
        if DATE_LINE_RE.match(lines[j].strip()):
            return True
        if ENTRY_START_RE.match(lines[j].strip()):
            return True
    return False


def choose_alts_from_text(text: str, limit: int = 3) -> list[str]:
    out: list[str] = []
    for u in extract_urls(text):
        if is_skipped(u):
            continue
        if u not in out:
            out.append(u)
        if len(out) >= limit:
            break
    return out


def slug_from_path(rel: str) -> str | None:
    p = Path(rel)
    if p.parts[:2] == ("pages", "works") and p.suffix == ".md":
        return p.stem
    if p.parts[:2] == ("pages", "brands") and p.suffix == ".md":
        return p.stem
    return None


def related_candidates(slug: str) -> list[str]:
    # Search a few key pages for non-skipped URLs and return a small set.
    candidates: list[str] = []
    related_files = [
        BASE / "pages" / "filmography.md",
        BASE / "pages" / "schedule.md",
        BASE / "pages" / "pictorials" / "by-year.md",
        BASE / "pages" / "appearances" / "by-year.md",
    ]
    for f in related_files:
        if not f.exists():
            continue
        txt = f.read_text(encoding="utf-8", errors="ignore")
        if slug.replace("-", " ") not in txt and slug not in txt:
            # lightweight filter: if slug not mentioned, still allow filmography/schedule as global proof pool
            if f.name not in {"filmography.md", "schedule.md"}:
                continue
        for u in extract_urls(txt):
            if is_skipped(u):
                continue
            if u not in candidates:
                candidates.append(u)
            if len(candidates) >= 5:
                return candidates[:5]
    return candidates[:5]


def apply_one(path: Path, missing: Missing) -> int:
    rel = path.relative_to(BASE).as_posix()
    if rel != missing.file:
        return 0

    lines = path.read_text(encoding="utf-8", errors="ignore").splitlines()
    changed = 0

    for idx, ln in enumerate(lines):
        if missing.url not in ln:
            continue
        if has_alt_near(lines, idx + 1):
            continue

        # 1) same file alts
        alts = choose_alts_from_text("\n".join(lines))

        # 2) related alts
        if len(alts) < 1:
            slug = slug_from_path(rel)
            if slug:
                alts = related_candidates(slug)

        if not alts:
            continue

        alt_text = ", ".join(alts[:3])
        if in_structured_entry(lines, idx):
            lines.insert(idx + 1, f"- 대체 근거(열리는 링크): {alt_text}")
        else:
            lines[idx] = lines[idx] + f" <!-- ALT-PROOF: {' | '.join(alts[:3])} -->"
        changed += 1

    if changed:
        path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return changed


def main() -> None:
    if not DATA.exists():
        print("backfill_missing_alternative_proofs: missing json not found")
        return

    raw = json.loads(DATA.read_text(encoding="utf-8"))
    items = [Missing(file=str(it["file"]), line=int(it["line"]), url=str(it["url"])) for it in raw.get("items", [])]

    # Group by file
    by_file: dict[str, list[Missing]] = {}
    for it in items:
        by_file.setdefault(it.file, []).append(it)

    files_changed = 0
    inserts = 0

    for rel, arr in sorted(by_file.items()):
        p = BASE / rel
        if not p.exists():
            continue
        before = 0
        for m in arr:
            before += apply_one(p, m)
        if before:
            files_changed += 1
            inserts += before

    print(f"backfill_missing_alternative_proofs: files_changed={files_changed} inserts={inserts} total_missing={len(items)}")


if __name__ == "__main__":
    main()
